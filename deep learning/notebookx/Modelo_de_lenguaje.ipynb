{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modelo de lenguaje.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.- Instalar Dependencias"
      ],
      "metadata": {
        "id": "pst9b4fYTgkj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TebXEO4F-FNs",
        "outputId": "6f6a4789-0d75-4410-a81e-f425142aa911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                       Version\n",
            "----------------------------- ---------------------\n",
            "absl-py                       1.0.0\n",
            "alabaster                     0.7.12\n",
            "albumentations                0.1.12\n",
            "altair                        4.2.0\n",
            "appdirs                       1.4.4\n",
            "argon2-cffi                   21.3.0\n",
            "argon2-cffi-bindings          21.2.0\n",
            "arviz                         0.12.0\n",
            "astor                         0.8.1\n",
            "astropy                       4.3.1\n",
            "astunparse                    1.6.3\n",
            "atari-py                      0.2.9\n",
            "atomicwrites                  1.4.0\n",
            "attrs                         21.4.0\n",
            "audioread                     2.1.9\n",
            "autograd                      1.4\n",
            "Babel                         2.10.1\n",
            "backcall                      0.2.0\n",
            "beautifulsoup4                4.6.3\n",
            "bleach                        5.0.0\n",
            "blis                          0.4.1\n",
            "bokeh                         2.3.3\n",
            "Bottleneck                    1.3.4\n",
            "branca                        0.5.0\n",
            "bs4                           0.0.1\n",
            "CacheControl                  0.12.11\n",
            "cached-property               1.5.2\n",
            "cachetools                    4.2.4\n",
            "catalogue                     1.0.0\n",
            "certifi                       2021.10.8\n",
            "cffi                          1.15.0\n",
            "cftime                        1.6.0\n",
            "chardet                       3.0.4\n",
            "charset-normalizer            2.0.12\n",
            "click                         7.1.2\n",
            "cloudpickle                   1.3.0\n",
            "cmake                         3.22.4\n",
            "cmdstanpy                     0.9.5\n",
            "colorcet                      3.0.0\n",
            "colorlover                    0.3.0\n",
            "community                     1.0.0b1\n",
            "contextlib2                   0.5.5\n",
            "convertdate                   2.4.0\n",
            "coverage                      3.7.1\n",
            "coveralls                     0.5\n",
            "crcmod                        1.7\n",
            "cufflinks                     0.17.3\n",
            "cvxopt                        1.2.7\n",
            "cvxpy                         1.0.31\n",
            "cycler                        0.11.0\n",
            "cymem                         2.0.6\n",
            "Cython                        0.29.28\n",
            "daft                          0.0.4\n",
            "dask                          2.12.0\n",
            "datascience                   0.10.6\n",
            "debugpy                       1.0.0\n",
            "decorator                     4.4.2\n",
            "defusedxml                    0.7.1\n",
            "descartes                     1.1.0\n",
            "dill                          0.3.4\n",
            "distributed                   1.25.3\n",
            "dlib                          19.18.0\n",
            "dm-tree                       0.1.7\n",
            "docopt                        0.6.2\n",
            "docutils                      0.17.1\n",
            "dopamine-rl                   1.0.5\n",
            "earthengine-api               0.1.306\n",
            "easydict                      1.9\n",
            "ecos                          2.0.10\n",
            "editdistance                  0.5.3\n",
            "en-core-web-sm                2.2.5\n",
            "entrypoints                   0.4\n",
            "ephem                         4.1.3\n",
            "et-xmlfile                    1.1.0\n",
            "fa2                           0.3.5\n",
            "fastai                        1.0.61\n",
            "fastdtw                       0.3.4\n",
            "fastjsonschema                2.15.3\n",
            "fastprogress                  1.0.2\n",
            "fastrlock                     0.8\n",
            "fbprophet                     0.7.1\n",
            "feather-format                0.4.1\n",
            "filelock                      3.6.0\n",
            "firebase-admin                4.4.0\n",
            "fix-yahoo-finance             0.0.22\n",
            "Flask                         1.1.4\n",
            "flatbuffers                   2.0\n",
            "folium                        0.8.3\n",
            "future                        0.16.0\n",
            "gast                          0.5.3\n",
            "GDAL                          2.2.2\n",
            "gdown                         4.4.0\n",
            "gensim                        3.6.0\n",
            "geographiclib                 1.52\n",
            "geopy                         1.17.0\n",
            "gin-config                    0.5.0\n",
            "glob2                         0.7\n",
            "google                        2.0.3\n",
            "google-api-core               1.31.5\n",
            "google-api-python-client      1.12.11\n",
            "google-auth                   1.35.0\n",
            "google-auth-httplib2          0.0.4\n",
            "google-auth-oauthlib          0.4.6\n",
            "google-cloud-bigquery         1.21.0\n",
            "google-cloud-bigquery-storage 1.1.1\n",
            "google-cloud-core             1.0.3\n",
            "google-cloud-datastore        1.8.0\n",
            "google-cloud-firestore        1.7.0\n",
            "google-cloud-language         1.2.0\n",
            "google-cloud-storage          1.18.1\n",
            "google-cloud-translate        1.5.0\n",
            "google-colab                  1.0.0\n",
            "google-pasta                  0.2.0\n",
            "google-resumable-media        0.4.1\n",
            "googleapis-common-protos      1.56.0\n",
            "googledrivedownloader         0.4\n",
            "graphviz                      0.10.1\n",
            "greenlet                      1.1.2\n",
            "grpcio                        1.44.0\n",
            "gspread                       3.4.2\n",
            "gspread-dataframe             3.0.8\n",
            "gym                           0.17.3\n",
            "h5py                          3.1.0\n",
            "HeapDict                      1.0.1\n",
            "hijri-converter               2.2.3\n",
            "holidays                      0.10.5.2\n",
            "holoviews                     1.14.8\n",
            "html5lib                      1.0.1\n",
            "httpimport                    0.5.18\n",
            "httplib2                      0.17.4\n",
            "httplib2shim                  0.0.3\n",
            "humanize                      0.5.1\n",
            "hyperopt                      0.1.2\n",
            "ideep4py                      2.0.0.post3\n",
            "idna                          2.10\n",
            "imageio                       2.4.1\n",
            "imagesize                     1.3.0\n",
            "imbalanced-learn              0.8.1\n",
            "imblearn                      0.0\n",
            "imgaug                        0.2.9\n",
            "importlib-metadata            4.11.3\n",
            "importlib-resources           5.7.1\n",
            "imutils                       0.5.4\n",
            "inflect                       2.1.0\n",
            "iniconfig                     1.1.1\n",
            "intel-openmp                  2022.0.2\n",
            "intervaltree                  2.1.0\n",
            "ipykernel                     4.10.1\n",
            "ipython                       5.5.0\n",
            "ipython-genutils              0.2.0\n",
            "ipython-sql                   0.3.9\n",
            "ipywidgets                    7.7.0\n",
            "itsdangerous                  1.1.0\n",
            "jax                           0.3.4\n",
            "jaxlib                        0.3.2+cuda11.cudnn805\n",
            "jedi                          0.18.1\n",
            "jieba                         0.42.1\n",
            "Jinja2                        2.11.3\n",
            "joblib                        1.1.0\n",
            "jpeg4py                       0.1.4\n",
            "jsonschema                    4.3.3\n",
            "jupyter                       1.0.0\n",
            "jupyter-client                5.3.5\n",
            "jupyter-console               5.2.0\n",
            "jupyter-core                  4.10.0\n",
            "jupyterlab-pygments           0.2.2\n",
            "jupyterlab-widgets            1.1.0\n",
            "kaggle                        1.5.12\n",
            "kapre                         0.3.7\n",
            "keras                         2.8.0\n",
            "Keras-Preprocessing           1.1.2\n",
            "keras-vis                     0.4.1\n",
            "kiwisolver                    1.4.2\n",
            "korean-lunar-calendar         0.2.1\n",
            "libclang                      14.0.1\n",
            "librosa                       0.8.1\n",
            "lightgbm                      2.2.3\n",
            "llvmlite                      0.34.0\n",
            "lmdb                          0.99\n",
            "LunarCalendar                 0.0.9\n",
            "lxml                          4.2.6\n",
            "Markdown                      3.3.6\n",
            "MarkupSafe                    2.0.1\n",
            "matplotlib                    3.2.2\n",
            "matplotlib-inline             0.1.3\n",
            "matplotlib-venn               0.11.7\n",
            "missingno                     0.5.1\n",
            "mistune                       0.8.4\n",
            "mizani                        0.6.0\n",
            "mkl                           2019.0\n",
            "mlxtend                       0.14.0\n",
            "more-itertools                8.12.0\n",
            "moviepy                       0.2.3.5\n",
            "mpmath                        1.2.1\n",
            "msgpack                       1.0.3\n",
            "multiprocess                  0.70.12.2\n",
            "multitasking                  0.0.10\n",
            "murmurhash                    1.0.7\n",
            "music21                       5.5.0\n",
            "natsort                       5.5.0\n",
            "nbclient                      0.6.0\n",
            "nbconvert                     5.6.1\n",
            "nbformat                      5.3.0\n",
            "nest-asyncio                  1.5.5\n",
            "netCDF4                       1.5.8\n",
            "networkx                      2.6.3\n",
            "nibabel                       3.0.2\n",
            "nltk                          3.2.5\n",
            "notebook                      5.3.1\n",
            "numba                         0.51.2\n",
            "numexpr                       2.8.1\n",
            "numpy                         1.21.6\n",
            "nvidia-ml-py3                 7.352.0\n",
            "oauth2client                  4.1.3\n",
            "oauthlib                      3.2.0\n",
            "okgrade                       0.4.3\n",
            "opencv-contrib-python         4.1.2.30\n",
            "opencv-python                 4.1.2.30\n",
            "openpyxl                      3.0.9\n",
            "opt-einsum                    3.3.0\n",
            "osqp                          0.6.2.post0\n",
            "packaging                     21.3\n",
            "palettable                    3.3.0\n",
            "pandas                        1.3.5\n",
            "pandas-datareader             0.9.0\n",
            "pandas-gbq                    0.13.3\n",
            "pandas-profiling              1.4.1\n",
            "pandocfilters                 1.5.0\n",
            "panel                         0.12.1\n",
            "param                         1.12.1\n",
            "parso                         0.8.3\n",
            "pathlib                       1.0.1\n",
            "patsy                         0.5.2\n",
            "pep517                        0.12.0\n",
            "pexpect                       4.8.0\n",
            "pickleshare                   0.7.5\n",
            "Pillow                        7.1.2\n",
            "pip                           21.1.3\n",
            "pip-tools                     6.2.0\n",
            "plac                          1.1.3\n",
            "plotly                        5.5.0\n",
            "plotnine                      0.6.0\n",
            "pluggy                        0.7.1\n",
            "pooch                         1.6.0\n",
            "portpicker                    1.3.9\n",
            "prefetch-generator            1.0.1\n",
            "preshed                       3.0.6\n",
            "prettytable                   3.2.0\n",
            "progressbar2                  3.38.0\n",
            "prometheus-client             0.14.1\n",
            "promise                       2.3\n",
            "prompt-toolkit                1.0.18\n",
            "protobuf                      3.17.3\n",
            "psutil                        5.4.8\n",
            "psycopg2                      2.7.6.1\n",
            "ptyprocess                    0.7.0\n",
            "py                            1.11.0\n",
            "pyarrow                       6.0.1\n",
            "pyasn1                        0.4.8\n",
            "pyasn1-modules                0.2.8\n",
            "pycocotools                   2.0.4\n",
            "pycparser                     2.21\n",
            "pyct                          0.4.8\n",
            "pydata-google-auth            1.4.0\n",
            "pydot                         1.3.0\n",
            "pydot-ng                      2.0.0\n",
            "pydotplus                     2.0.2\n",
            "PyDrive                       1.3.1\n",
            "pyemd                         0.5.1\n",
            "pyerfa                        2.0.0.1\n",
            "pyglet                        1.5.0\n",
            "Pygments                      2.6.1\n",
            "pygobject                     3.26.1\n",
            "pymc3                         3.11.4\n",
            "PyMeeus                       0.5.11\n",
            "pymongo                       4.1.1\n",
            "pymystem3                     0.2.0\n",
            "PyOpenGL                      3.1.6\n",
            "pyparsing                     3.0.8\n",
            "pyrsistent                    0.18.1\n",
            "pysndfile                     1.3.8\n",
            "PySocks                       1.7.1\n",
            "pystan                        2.19.1.1\n",
            "pytest                        3.6.4\n",
            "python-apt                    0.0.0\n",
            "python-chess                  0.23.11\n",
            "python-dateutil               2.8.2\n",
            "python-louvain                0.16\n",
            "python-slugify                6.1.2\n",
            "python-utils                  3.1.0\n",
            "pytz                          2022.1\n",
            "pyviz-comms                   2.2.0\n",
            "PyWavelets                    1.3.0\n",
            "PyYAML                        3.13\n",
            "pyzmq                         22.3.0\n",
            "qdldl                         0.1.5.post2\n",
            "qtconsole                     5.3.0\n",
            "QtPy                          2.0.1\n",
            "regex                         2019.12.20\n",
            "requests                      2.23.0\n",
            "requests-oauthlib             1.3.1\n",
            "resampy                       0.2.2\n",
            "rpy2                          3.4.5\n",
            "rsa                           4.8\n",
            "scikit-image                  0.18.3\n",
            "scikit-learn                  1.0.2\n",
            "scipy                         1.4.1\n",
            "screen-resolution-extra       0.0.0\n",
            "scs                           3.2.0\n",
            "seaborn                       0.11.2\n",
            "semver                        2.13.0\n",
            "Send2Trash                    1.8.0\n",
            "setuptools                    57.4.0\n",
            "setuptools-git                1.2\n",
            "Shapely                       1.8.1.post1\n",
            "simplegeneric                 0.8.1\n",
            "six                           1.15.0\n",
            "sklearn                       0.0\n",
            "sklearn-pandas                1.8.0\n",
            "smart-open                    6.0.0\n",
            "snowballstemmer               2.2.0\n",
            "sortedcontainers              2.4.0\n",
            "SoundFile                     0.10.3.post1\n",
            "soupsieve                     2.3.2.post1\n",
            "spacy                         2.2.4\n",
            "Sphinx                        1.8.6\n",
            "sphinxcontrib-serializinghtml 1.1.5\n",
            "sphinxcontrib-websupport      1.2.4\n",
            "SQLAlchemy                    1.4.36\n",
            "sqlparse                      0.4.2\n",
            "srsly                         1.0.5\n",
            "statsmodels                   0.10.2\n",
            "sympy                         1.7.1\n",
            "tables                        3.7.0\n",
            "tabulate                      0.8.9\n",
            "tblib                         1.7.0\n",
            "tenacity                      8.0.1\n",
            "tensorboard                   2.8.0\n",
            "tensorboard-data-server       0.6.1\n",
            "tensorboard-plugin-wit        1.8.1\n",
            "tensorflow                    2.8.0\n",
            "tensorflow-datasets           4.0.1\n",
            "tensorflow-estimator          2.8.0\n",
            "tensorflow-gcs-config         2.8.0\n",
            "tensorflow-hub                0.12.0\n",
            "tensorflow-io-gcs-filesystem  0.25.0\n",
            "tensorflow-metadata           1.7.0\n",
            "tensorflow-probability        0.16.0\n",
            "termcolor                     1.1.0\n",
            "terminado                     0.13.3\n",
            "testpath                      0.6.0\n",
            "text-unidecode                1.3\n",
            "textblob                      0.15.3\n",
            "Theano-PyMC                   1.1.2\n",
            "thinc                         7.4.0\n",
            "threadpoolctl                 3.1.0\n",
            "tifffile                      2021.11.2\n",
            "tinycss2                      1.1.1\n",
            "tomli                         2.0.1\n",
            "toolz                         0.11.2\n",
            "torch                         1.11.0+cu113\n",
            "torchaudio                    0.11.0+cu113\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.12.0\n",
            "torchvision                   0.12.0+cu113\n",
            "tornado                       5.1.1\n",
            "tqdm                          4.64.0\n",
            "traitlets                     5.1.1\n",
            "tweepy                        3.10.0\n",
            "typeguard                     2.7.1\n",
            "typing-extensions             4.2.0\n",
            "tzlocal                       1.5.1\n",
            "uritemplate                   3.0.1\n",
            "urllib3                       1.24.3\n",
            "vega-datasets                 0.9.0\n",
            "wasabi                        0.9.1\n",
            "wcwidth                       0.2.5\n",
            "webencodings                  0.5.1\n",
            "Werkzeug                      1.0.1\n",
            "wheel                         0.37.1\n",
            "widgetsnbextension            3.6.0\n",
            "wordcloud                     1.5.0\n",
            "wrapt                         1.14.0\n",
            "xarray                        0.18.2\n",
            "xgboost                       0.90\n",
            "xkit                          0.0.0\n",
            "xlrd                          1.1.0\n",
            "xlwt                          1.3.0\n",
            "yellowbrick                   1.4\n",
            "zict                          2.1.0\n",
            "zipp                          3.8.0\n"
          ]
        }
      ],
      "source": [
        "#se requiere tener transformers 2.8.0\n",
        "!pip list #devuelve la lista de paquetes en el entorno actual junto con la versión de cada paquete"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip list | grep -E 'tensorflow|transformers' #verificamos especificamente los paquetes de tensorflow y transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jF1H-DPPJNi",
        "outputId": "203b6895-33a9-4732-a1a5-7dfe6a1826e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow                    2.8.0\n",
            "tensorflow-datasets           4.0.1\n",
            "tensorflow-estimator          2.8.0\n",
            "tensorflow-gcs-config         2.8.0\n",
            "tensorflow-hub                0.12.0\n",
            "tensorflow-io-gcs-filesystem  0.25.0\n",
            "tensorflow-metadata           1.7.0\n",
            "tensorflow-probability        0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install transformers==2.8.0\n",
        "# tensorflow y transformers chocan"
      ],
      "metadata": {
        "id": "P7knicwFRJ_d"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Obtener datos"
      ],
      "metadata": {
        "id": "1OnxQLvkZDxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Bajar y descomprimir el conjunto de  datos de subtitulos de las peliculas\n",
        "if not os.path.exists('data/dataset.txt'):\n",
        "  !wget \"https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/es.txt.gz\" -O dataset.txt.gz\n",
        "  !gzip -d dataset.txt.gz\n",
        "  !mkdir data\n",
        "  !mv dataset.txt data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jE6y-1nRH9n",
        "outputId": "90abf3f4-db03-464d-9b4b-cb7c97313817"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-30 14:39:05--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/es.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1859673728 (1.7G) [application/gzip]\n",
            "Saving to: ‘dataset.txt.gz’\n",
            "\n",
            "dataset.txt.gz      100%[===================>]   1.73G  20.3MB/s    in 86s     \n",
            "\n",
            "2022-04-30 14:40:32 (20.6 MB/s) - ‘dataset.txt.gz’ saved [1859673728/1859673728]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar el número toal Total de líneas de los datos y visualizar algunos de los datos\n",
        "!wc -l data/dataset.txt\n",
        "!shuf -n 5 data/dataset.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exxEb3RXSNe9",
        "outputId": "96712d12-d75a-4801-bbd9-68c656913be4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179287150 data/dataset.txt\n",
            "Michael, ¿vas entendiendo?\n",
            "¿Sabes qué?\n",
            "Bueno, para empezar, no sé dónde has estado.\n",
            "Aún te queda uno\n",
            "Ken, ¿estás bien?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener un subconjunto del primer 1,000,000 de lineas para el entrenamiento\n",
        "TRAIN_SIZE = 1000000 #@param {type:\"integer\"}\n",
        "!(head -n $TRAIN_SIZE data/dataset.txt) > data/train.txt"
      ],
      "metadata": {
        "id": "FyRhCaBnTRU8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener un subconjunto de las siguientes 10,000 lineas para la validación \n",
        "VAL_SIZE = 10000 #@param {type:\"integer\"}\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p data/dataset.txt) > data/dev.txt"
      ],
      "metadata": {
        "id": "Ce_QTVMtTS5V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenar el tokenizador"
      ],
      "metadata": {
        "id": "9CbzE-D9ZSwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "path = \"data/train.txt\"\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=path,\n",
        "                vocab_size=50265,\n",
        "                min_frequency=2,\n",
        "                special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "\n",
        "# Save files to disk\n",
        "!mkdir -p \"models/roberta\"\n",
        "tokenizer.save(\"models/roberta\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTnA4F6WZXE-",
        "outputId": "b92eff67-b457-46ed-bc58-50009bba3f6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26.7 s, sys: 398 ms, total: 27.1 s\n",
            "Wall time: 26.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenamiento del modelo "
      ],
      "metadata": {
        "id": "A8Y49OydbS1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Arquitectura del modelo"
      ],
      "metadata": {
        "id": "9tUagVGWbehQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "config = {\n",
        "\t\"architectures\": [\n",
        "\t\t\"RobertaForMaskedLM\"\n",
        "\t],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"roberta\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 12,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": 50265\n",
        "}\n",
        "\n",
        "with open(\"models/roberta/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "tokenizer_config = {\"max_len\": 512}\n",
        "\n",
        "with open(\"models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)"
      ],
      "metadata": {
        "id": "0nP0VZtJaHZO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Inicio del entrenamiento"
      ],
      "metadata": {
        "id": "wRV_E40wcaq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update April 22, 2020: Hugging Face updated run_language_modeling.py script.\n",
        "# Please use this version which was before the update.\n",
        "!wget -c https://raw.githubusercontent.com/chriskhanhtran/spanish-bert/master/run_language_modeling.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt70emTockQY",
        "outputId": "0e9357cf-b917-4f0a-a87a-ead4f8bf1caf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-30 14:46:38--  https://raw.githubusercontent.com/chriskhanhtran/spanish-bert/master/run_language_modeling.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34328 (34K) [text/plain]\n",
            "Saving to: ‘run_language_modeling.py’\n",
            "\n",
            "\rrun_language_modeli   0%[                    ]       0  --.-KB/s               \rrun_language_modeli 100%[===================>]  33.52K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-04-30 14:46:38 (13.7 MB/s) - ‘run_language_modeling.py’ saved [34328/34328]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Argumentos importantes"
      ],
      "metadata": {
        "id": "jozoraq9dCnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model paths\n",
        "MODEL_TYPE = \"roberta\" #@param [\"roberta\", \"bert\"]\n",
        "MODEL_DIR = \"models/roberta\" #@param {type: \"string\"}\n",
        "OUTPUT_DIR = \"models/roberta/output\" #@param {type: \"string\"}\n",
        "TRAIN_PATH = \"data/train.txt\" #@param {type: \"string\"}\n",
        "EVAL_PATH = \"data/dev.txt\" #@param {type: \"string\"}"
      ],
      "metadata": {
        "id": "yUXr2340dHjC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoscebMYdRO8",
        "outputId": "cb5f04c5-c849-4fc1-b202-6961ab75660e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr 30 14:48:15 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Command line\n",
        "cmd = \"\"\"python run_language_modeling.py \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --model_type {model_type} \\\n",
        "    --mlm \\\n",
        "    --config_name {config_name} \\\n",
        "    --tokenizer_name {tokenizer_name} \\\n",
        "    {line_by_line} \\\n",
        "    {should_continue} \\\n",
        "    {model_name_or_path} \\\n",
        "    --train_data_file {train_path} \\\n",
        "    --eval_data_file {eval_path} \\\n",
        "    --do_train \\\n",
        "    {do_eval} \\\n",
        "    {evaluate_during_training} \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size 512 \\\n",
        "    --max_step 25 \\\n",
        "    --warmup_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --adam_epsilon 1e-6 \\\n",
        "    --max_grad_norm 100.0 \\\n",
        "    --save_total_limit 10 \\\n",
        "    --save_steps 10 \\\n",
        "    --logging_steps 2 \\\n",
        "    --seed 42\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "v7tBkKL9dpXk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments for training from scratch. I turn off evaluate_during_training,\n",
        "#   line_by_line, should_continue, and model_name_or_path.\n",
        "train_params = {\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"model_type\": MODEL_TYPE,\n",
        "    \"config_name\": MODEL_DIR,\n",
        "    \"tokenizer_name\": MODEL_DIR,\n",
        "    \"train_path\": TRAIN_PATH,\n",
        "    \"eval_path\": EVAL_PATH,\n",
        "    \"do_eval\": \"--do_eval\",\n",
        "    \"evaluate_during_training\": \"\",\n",
        "    \"line_by_line\": \"\",\n",
        "    \"should_continue\": \"\",\n",
        "    \"model_name_or_path\": \"\",\n",
        "}"
      ],
      "metadata": {
        "id": "7sOKeDFtdumI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!{cmd.format(**train_params)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XoCPnKDd25X",
        "outputId": "6ab46241-c687-471e-f48e-ccc12b682b75"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04/30/2022 14:48:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/30/2022 14:48:47 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/config.json\n",
            "04/30/2022 14:48:47 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/30/2022 14:48:47 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/config.json\n",
            "04/30/2022 14:48:47 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/30/2022 14:48:47 - INFO - transformers.tokenization_utils -   Model name 'models/roberta' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'models/roberta' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/30/2022 14:48:47 - INFO - transformers.tokenization_utils -   Didn't find file models/roberta/added_tokens.json. We won't load it.\n",
            "04/30/2022 14:48:47 - INFO - transformers.tokenization_utils -   Didn't find file models/roberta/special_tokens_map.json. We won't load it.\n",
            "04/30/2022 14:48:47 - INFO - transformers.tokenization_utils -   loading file models/roberta/vocab.json\n",
            "04/30/2022 14:48:47 - INFO - transformers.tokenization_utils -   loading file models/roberta/merges.txt\n",
            "04/30/2022 14:48:47 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/30/2022 14:48:47 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/30/2022 14:48:47 - INFO - transformers.tokenization_utils -   loading file models/roberta/tokenizer_config.json\n",
            "04/30/2022 14:48:47 - INFO - __main__ -   Training new model from scratch\n",
            "04/30/2022 14:49:04 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='data/dev.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path=None, model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='models/roberta/output', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='models/roberta', train_data_file='data/train.txt', warmup_steps=10, weight_decay=0.01)\n",
            "04/30/2022 14:49:04 - INFO - __main__ -   Creating features from dataset file at data\n",
            "04/30/2022 14:49:40 - INFO - __main__ -   Saving features into cached file data/roberta_cached_lm_510_train.txt\n",
            "04/30/2022 14:49:40 - INFO - __main__ -   ***** Running training *****\n",
            "04/30/2022 14:49:40 - INFO - __main__ -     Num examples = 15932\n",
            "04/30/2022 14:49:40 - INFO - __main__ -     Num Epochs = 1\n",
            "04/30/2022 14:49:40 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "04/30/2022 14:49:40 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "04/30/2022 14:49:40 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
            "04/30/2022 14:49:40 - INFO - __main__ -     Total optimization steps = 25\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/3983 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/3983 [00:01<1:18:29,  1.18s/it]\u001b[A\n",
            "Iteration:   0% 2/3983 [00:02<1:13:46,  1.11s/it]\u001b[A\n",
            "Iteration:   0% 3/3983 [00:03<1:12:27,  1.09s/it]\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   0% 4/3983 [00:04<1:14:41,  1.13s/it]\u001b[A\n",
            "Iteration:   0% 5/3983 [00:05<1:12:39,  1.10s/it]\u001b[A\n",
            "Iteration:   0% 6/3983 [00:06<1:12:05,  1.09s/it]\u001b[A\n",
            "Iteration:   0% 7/3983 [00:07<1:11:46,  1.08s/it]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:   0% 8/3983 [00:08<1:13:15,  1.11s/it]\u001b[A\n",
            "Iteration:   0% 9/3983 [00:09<1:12:23,  1.09s/it]\u001b[A\n",
            "Iteration:   0% 10/3983 [00:10<1:11:44,  1.08s/it]\u001b[A\n",
            "Iteration:   0% 11/3983 [00:12<1:11:43,  1.08s/it]\u001b[A\n",
            "Iteration:   0% 12/3983 [00:13<1:12:54,  1.10s/it]\u001b[A\n",
            "Iteration:   0% 13/3983 [00:14<1:11:56,  1.09s/it]\u001b[A\n",
            "Iteration:   0% 14/3983 [00:15<1:11:11,  1.08s/it]\u001b[A\n",
            "Iteration:   0% 15/3983 [00:16<1:11:10,  1.08s/it]\u001b[A\n",
            "Iteration:   0% 16/3983 [00:17<1:12:46,  1.10s/it]\u001b[A\n",
            "Iteration:   0% 17/3983 [00:18<1:11:54,  1.09s/it]\u001b[A\n",
            "Iteration:   0% 18/3983 [00:19<1:11:13,  1.08s/it]\u001b[A\n",
            "Iteration:   0% 19/3983 [00:20<1:10:46,  1.07s/it]\u001b[A\n",
            "Iteration:   1% 20/3983 [00:21<1:12:29,  1.10s/it]\u001b[A\n",
            "Iteration:   1% 21/3983 [00:22<1:11:51,  1.09s/it]\u001b[A\n",
            "Iteration:   1% 22/3983 [00:23<1:11:24,  1.08s/it]\u001b[A\n",
            "Iteration:   1% 23/3983 [00:25<1:11:09,  1.08s/it]\u001b[A\n",
            "Iteration:   1% 24/3983 [00:26<1:12:24,  1.10s/it]\u001b[A\n",
            "Iteration:   1% 25/3983 [00:27<1:11:22,  1.08s/it]\u001b[A\n",
            "Iteration:   1% 26/3983 [00:28<1:10:46,  1.07s/it]\u001b[A\n",
            "Iteration:   1% 27/3983 [00:29<1:10:32,  1.07s/it]\u001b[A\n",
            "Iteration:   1% 28/3983 [00:30<1:12:10,  1.09s/it]\u001b[A\n",
            "Iteration:   1% 29/3983 [00:31<1:11:01,  1.08s/it]\u001b[A\n",
            "Iteration:   1% 30/3983 [00:32<1:10:31,  1.07s/it]\u001b[A\n",
            "Iteration:   1% 31/3983 [00:33<1:10:04,  1.06s/it]\u001b[A\n",
            "Iteration:   1% 32/3983 [00:34<1:11:39,  1.09s/it]\u001b[A\n",
            "Iteration:   1% 33/3983 [00:35<1:10:41,  1.07s/it]\u001b[A\n",
            "Iteration:   1% 34/3983 [00:36<1:10:24,  1.07s/it]\u001b[A\n",
            "Iteration:   1% 35/3983 [00:37<1:10:20,  1.07s/it]\u001b[A\n",
            "Iteration:   1% 36/3983 [00:39<1:11:57,  1.09s/it]\u001b[A\n",
            "Iteration:   1% 37/3983 [00:40<1:11:15,  1.08s/it]\u001b[A\n",
            "Iteration:   1% 38/3983 [00:41<1:10:33,  1.07s/it]\u001b[A\n",
            "Iteration:   1% 39/3983 [00:42<1:10:00,  1.06s/it]\u001b[A04/30/2022 14:50:24 - INFO - transformers.configuration_utils -   Configuration saved in models/roberta/output/checkpoint-10/config.json\n",
            "04/30/2022 14:50:25 - INFO - transformers.modeling_utils -   Model weights saved in models/roberta/output/checkpoint-10/pytorch_model.bin\n",
            "04/30/2022 14:50:25 - INFO - __main__ -   Saving model checkpoint to models/roberta/output/checkpoint-10\n",
            "04/30/2022 14:50:29 - INFO - __main__ -   Saving optimizer and scheduler states to models/roberta/output/checkpoint-10\n",
            "\n",
            "Iteration:   1% 40/3983 [00:48<2:55:38,  2.67s/it]\u001b[A\n",
            "Iteration:   1% 41/3983 [00:49<2:24:32,  2.20s/it]\u001b[A\n",
            "Iteration:   1% 42/3983 [00:50<2:01:52,  1.86s/it]\u001b[A\n",
            "Iteration:   1% 43/3983 [00:51<1:46:09,  1.62s/it]\u001b[A\n",
            "Iteration:   1% 44/3983 [00:53<1:36:55,  1.48s/it]\u001b[A\n",
            "Iteration:   1% 45/3983 [00:54<1:28:40,  1.35s/it]\u001b[A\n",
            "Iteration:   1% 46/3983 [00:55<1:22:44,  1.26s/it]\u001b[A\n",
            "Iteration:   1% 47/3983 [00:56<1:18:57,  1.20s/it]\u001b[A\n",
            "Iteration:   1% 48/3983 [00:57<1:17:45,  1.19s/it]\u001b[A\n",
            "Iteration:   1% 49/3983 [00:58<1:15:19,  1.15s/it]\u001b[A\n",
            "Iteration:   1% 50/3983 [00:59<1:13:32,  1.12s/it]\u001b[A\n",
            "Iteration:   1% 51/3983 [01:00<1:12:18,  1.10s/it]\u001b[A\n",
            "Iteration:   1% 52/3983 [01:01<1:13:15,  1.12s/it]\u001b[A\n",
            "Iteration:   1% 53/3983 [01:02<1:12:03,  1.10s/it]\u001b[A\n",
            "Iteration:   1% 54/3983 [01:03<1:11:06,  1.09s/it]\u001b[A\n",
            "Iteration:   1% 55/3983 [01:04<1:10:33,  1.08s/it]\u001b[A\n",
            "Iteration:   1% 56/3983 [01:06<1:11:54,  1.10s/it]\u001b[A\n",
            "Iteration:   1% 57/3983 [01:07<1:10:47,  1.08s/it]\u001b[A\n",
            "Iteration:   1% 58/3983 [01:08<1:10:14,  1.07s/it]\u001b[A\n",
            "Iteration:   1% 59/3983 [01:09<1:09:40,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 60/3983 [01:10<1:10:59,  1.09s/it]\u001b[A\n",
            "Iteration:   2% 61/3983 [01:11<1:10:21,  1.08s/it]\u001b[A\n",
            "Iteration:   2% 62/3983 [01:12<1:09:35,  1.06s/it]\u001b[A\n",
            "Iteration:   2% 63/3983 [01:13<1:09:35,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 64/3983 [01:14<1:10:51,  1.08s/it]\u001b[A\n",
            "Iteration:   2% 65/3983 [01:15<1:10:15,  1.08s/it]\u001b[A\n",
            "Iteration:   2% 66/3983 [01:16<1:09:36,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 67/3983 [01:17<1:09:34,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 68/3983 [01:18<1:10:50,  1.09s/it]\u001b[A\n",
            "Iteration:   2% 69/3983 [01:19<1:09:55,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 70/3983 [01:20<1:09:35,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 71/3983 [01:22<1:09:30,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 72/3983 [01:23<1:10:53,  1.09s/it]\u001b[A\n",
            "Iteration:   2% 73/3983 [01:24<1:09:54,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 74/3983 [01:25<1:09:24,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 75/3983 [01:26<1:09:05,  1.06s/it]\u001b[A\n",
            "Iteration:   2% 76/3983 [01:27<1:10:21,  1.08s/it]\u001b[A\n",
            "Iteration:   2% 77/3983 [01:28<1:09:38,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 78/3983 [01:29<1:09:02,  1.06s/it]\u001b[A\n",
            "Iteration:   2% 79/3983 [01:30<1:08:35,  1.05s/it]\u001b[A04/30/2022 14:51:12 - INFO - transformers.configuration_utils -   Configuration saved in models/roberta/output/checkpoint-20/config.json\n",
            "04/30/2022 14:51:14 - INFO - transformers.modeling_utils -   Model weights saved in models/roberta/output/checkpoint-20/pytorch_model.bin\n",
            "04/30/2022 14:51:14 - INFO - __main__ -   Saving model checkpoint to models/roberta/output/checkpoint-20\n",
            "04/30/2022 14:51:18 - INFO - __main__ -   Saving optimizer and scheduler states to models/roberta/output/checkpoint-20\n",
            "\n",
            "Iteration:   2% 80/3983 [01:37<3:00:14,  2.77s/it]\u001b[A\n",
            "Iteration:   2% 81/3983 [01:38<2:27:26,  2.27s/it]\u001b[A\n",
            "Iteration:   2% 82/3983 [01:39<2:03:26,  1.90s/it]\u001b[A\n",
            "Iteration:   2% 83/3983 [01:40<1:46:42,  1.64s/it]\u001b[A\n",
            "Iteration:   2% 84/3983 [01:41<1:36:55,  1.49s/it]\u001b[A\n",
            "Iteration:   2% 85/3983 [01:42<1:28:14,  1.36s/it]\u001b[A\n",
            "Iteration:   2% 86/3983 [01:43<1:22:08,  1.26s/it]\u001b[A\n",
            "Iteration:   2% 87/3983 [01:44<1:18:05,  1.20s/it]\u001b[A\n",
            "Iteration:   2% 88/3983 [01:45<1:16:37,  1.18s/it]\u001b[A\n",
            "Iteration:   2% 89/3983 [01:46<1:14:00,  1.14s/it]\u001b[A\n",
            "Iteration:   2% 90/3983 [01:48<1:12:22,  1.12s/it]\u001b[A\n",
            "Iteration:   2% 91/3983 [01:49<1:11:10,  1.10s/it]\u001b[A\n",
            "Iteration:   2% 92/3983 [01:50<1:11:41,  1.11s/it]\u001b[A\n",
            "Iteration:   2% 93/3983 [01:51<1:10:19,  1.08s/it]\u001b[A\n",
            "Iteration:   2% 94/3983 [01:52<1:09:35,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 95/3983 [01:53<1:09:01,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 96/3983 [01:54<1:10:21,  1.09s/it]\u001b[A\n",
            "Iteration:   2% 97/3983 [01:55<1:09:31,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 98/3983 [01:56<1:09:05,  1.07s/it]\u001b[A\n",
            "Iteration:   2% 99/3983 [01:57<1:08:46,  1.06s/it]\u001b[A\n",
            "Iteration:   3% 100/3983 [01:58<1:09:55,  1.08s/it]\u001b[A\n",
            "Iteration:   3% 101/3983 [01:59<1:09:04,  1.07s/it]\u001b[A\n",
            "Iteration:   3% 102/3983 [02:00<1:08:54,  1.07s/it]\u001b[A\n",
            "Iteration:   3% 103/3983 [02:03<1:17:14,  1.19s/it]\n",
            "Epoch:   0% 0/1 [02:03<?, ?it/s]\n",
            "04/30/2022 14:51:44 - INFO - __main__ -    global_step = 26, average loss = 9.331250199904808\n",
            "04/30/2022 14:51:44 - INFO - __main__ -   Saving model checkpoint to models/roberta/output\n",
            "04/30/2022 14:51:44 - INFO - transformers.configuration_utils -   Configuration saved in models/roberta/output/config.json\n",
            "04/30/2022 14:51:45 - INFO - transformers.modeling_utils -   Model weights saved in models/roberta/output/pytorch_model.bin\n",
            "04/30/2022 14:51:45 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/output/config.json\n",
            "04/30/2022 14:51:45 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/30/2022 14:51:45 - INFO - transformers.modeling_utils -   loading weights file models/roberta/output/pytorch_model.bin\n",
            "04/30/2022 14:51:52 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/output/config.json\n",
            "04/30/2022 14:51:52 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/30/2022 14:51:52 - INFO - transformers.tokenization_utils -   Model name 'models/roberta/output' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'models/roberta/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/30/2022 14:51:52 - INFO - transformers.tokenization_utils -   Didn't find file models/roberta/output/added_tokens.json. We won't load it.\n",
            "04/30/2022 14:51:52 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/vocab.json\n",
            "04/30/2022 14:51:52 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/merges.txt\n",
            "04/30/2022 14:51:52 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/30/2022 14:51:52 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/special_tokens_map.json\n",
            "04/30/2022 14:51:52 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/tokenizer_config.json\n",
            "04/30/2022 14:51:52 - INFO - __main__ -   Evaluate the following checkpoints: ['models/roberta/output']\n",
            "04/30/2022 14:51:52 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/output/config.json\n",
            "04/30/2022 14:51:52 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/30/2022 14:51:52 - INFO - transformers.modeling_utils -   loading weights file models/roberta/output/pytorch_model.bin\n",
            "04/30/2022 14:51:58 - INFO - __main__ -   Creating features from dataset file at data\n",
            "04/30/2022 14:51:58 - INFO - __main__ -   Saving features into cached file data/roberta_cached_lm_510_dev.txt\n",
            "04/30/2022 14:51:58 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "04/30/2022 14:51:58 - INFO - __main__ -     Num examples = 180\n",
            "04/30/2022 14:51:58 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100% 45/45 [00:16<00:00,  2.72it/s]\n",
            "04/30/2022 14:52:15 - INFO - __main__ -   ***** Eval results  *****\n",
            "04/30/2022 14:52:15 - INFO - __main__ -     perplexity = tensor(7344.3716)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predecir palabras enmascaradas"
      ],
      "metadata": {
        "id": "ANkZko5VszzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%time\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"chriskhanhtran/spanberta\",\n",
        "    tokenizer=\"chriskhanhtran/spanberta\"\n",
        ")"
      ],
      "metadata": {
        "id": "_0sG9cKysfn4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"Lavarse frecuentemente las manos con agua y <mask>.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv2K5xfVtwcE",
        "outputId": "d7518e65-377d-4806-da7d-5b6e0bbb0c55"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.6469593644142151,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y jabón.</s>',\n",
              "  'token': 18493},\n",
              " {'score': 0.06074436753988266,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y sal.</s>',\n",
              "  'token': 619},\n",
              " {'score': 0.029788268730044365,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y vapor.</s>',\n",
              "  'token': 11079},\n",
              " {'score': 0.02641025371849537,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y limón.</s>',\n",
              "  'token': 12788},\n",
              " {'score': 0.017029233276844025,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y vinagre.</s>',\n",
              "  'token': 18424}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"Come frutas y <mask>.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyRiLB7iBzK5",
        "outputId": "bad7a74c-5c5b-4a6c-ec64-5522ed978e20"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5663056969642639,\n",
              "  'sequence': '<s> Come frutas y verduras.</s>',\n",
              "  'token': 10223},\n",
              " {'score': 0.24918697774410248,\n",
              "  'sequence': '<s> Come frutas y vegetales.</s>',\n",
              "  'token': 12582},\n",
              " {'score': 0.10890160501003265,\n",
              "  'sequence': '<s> Come frutas y hortalizas.</s>',\n",
              "  'token': 25283},\n",
              " {'score': 0.009446275420486927,\n",
              "  'sequence': '<s> Come frutas y verdura.</s>',\n",
              "  'token': 31641},\n",
              " {'score': 0.005149755626916885,\n",
              "  'sequence': '<s> Come frutas y bebidas.</s>',\n",
              "  'token': 8767}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"Estoy viendo <mask>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YE4eAeKB8_r",
        "outputId": "07bd74ce-2de4-4025-b88b-09b0dc35255f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.0782180055975914,\n",
              "  'sequence': '<s> Estoy viendo.</s>',\n",
              "  'token': 18},\n",
              " {'score': 0.04264475777745247,\n",
              "  'sequence': '<s> Estoy viendo Netflix</s>',\n",
              "  'token': 17125},\n",
              " {'score': 0.030524639412760735,\n",
              "  'sequence': '<s> Estoy viendo películas</s>',\n",
              "  'token': 4174},\n",
              " {'score': 0.0169787909835577,\n",
              "  'sequence': '<s> Estoy viendo Estoy</s>',\n",
              "  'token': 8679},\n",
              " {'score': 0.016631552949547768,\n",
              "  'sequence': '<s> Estoy viendo:</s>',\n",
              "  'token': 30}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}