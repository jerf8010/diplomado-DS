{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLN con Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leyendo el corpus A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_esA = pd.read_csv('public_development_esTaskA/train_es.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_esA = pd.read_csv('public_development_esTaskA/dev_es.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza en los datos\n",
    "* Cambiar todas las palabras de mayúsculas a minúsculas\n",
    "* Se han eliminado las '@' de @USUARIO con el fin de facilitar el etiquetado morfológico\n",
    "* Quitar los links \n",
    "* Quitar los emojis\n",
    "* Eliminar las stopwords\n",
    "* Se han reemplazado todos los números por el símbolo '0'\n",
    "* Quitar los signos de puntuación y quitar espacios (tabuladores, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_URL=\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\"\n",
    "\n",
    "def procesar(file, namefile):    \n",
    "    file[file.columns[1]] = [clean_text(i) for i in file[file.columns[1]]]    \n",
    "    file.to_csv(namefile, sep='\\t', encoding='utf-8', index=False)\n",
    "    return file\n",
    "    \n",
    "def clean_text(text):\n",
    "    text = text.lower()   \n",
    "    #text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \"@USUARIO\", text)\n",
    "    text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \" \", text)\n",
    "    text=re.sub(pattern_URL, \" \", text)\n",
    "    \n",
    "    text= remove_emoji(text)\n",
    "    text= remove_stopwords(text)\n",
    "    text=re.sub(\"\\d+\", \"0\", text)\n",
    "    # text=re.sub(\"\\d+\", \" \", text)\n",
    "    \n",
    "    text=re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", re.sub('(?:[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\'\\|])', \" \",text))))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):    \n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "    for i in stopwords:\n",
    "        text = re.sub(r\"\\b%s\\b\" % i, \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs                               \n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"\\U00002702-\\U000027B0\"\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"\\U0001f926-\\U0001f937\"\n",
    "                               \"\\u200d\"\n",
    "                               \"\\u2640-\\u2642\"\n",
    "                               \"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "                               \"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "                               \"\\U0001F600-\\U0001F64F\"\n",
    "                               \"\\U0001F1F2\"\n",
    "                               \"\\U0001F1F4\"\n",
    "                               \"\\U0001F620\"\n",
    "                               \"]+\", flags=re.UNICODE)   \n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando el corpus ya procesado A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_esA = procesar(corpus_train_esA, \"public_development_esTaskA/train_es_cleanA.tsv\")\n",
    "corpus_dev_esA = procesar(corpus_dev_esA, \"public_development_esTaskA/dev_es_cleanA.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo el corpus ya procesado-limpio A\n",
    "\n",
    "train_idA = corpus_train_esA[corpus_train_esA.columns[0]]\n",
    "X_train_textA = corpus_train_esA[corpus_train_esA.columns[1]].fillna(' ')\n",
    "y_train_hsA = corpus_train_esA[corpus_train_esA.columns[2]]\n",
    "\n",
    "test_idA = corpus_dev_esA[corpus_train_esA.columns[0]]\n",
    "X_test_textA = corpus_dev_esA[corpus_dev_esA.columns[1]].fillna(' ')\n",
    "y_test_hsA = corpus_dev_esA[corpus_dev_esA.columns[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4469 4469\n",
      "500 500\n"
     ]
    }
   ],
   "source": [
    "print( len(X_train_textA), len(y_train_hsA) )\n",
    "\n",
    "print( len(X_test_textA), len(y_test_hsA) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4469, 1775)\n",
      "(500, 1775)\n"
     ]
    }
   ],
   "source": [
    "cvectorizer = CountVectorizer(\n",
    "    # lowercase=True,\n",
    "    #stop_words=[word.decode('utf-8') for word in nltk.corpus.stopwords.words('spanish')],\n",
    "    #token_pattern=r'\\b\\w+\\b', #selects tokens of 2 or more alphanumeric characters \n",
    "    ngram_range=(1,3),#n-grams de palabras n = 1 a n = 3 (unigramas, bigramas y trigramas)\n",
    "    min_df=5,#ignorando los términos que tienen una frecuencia de documento estrictamente inferior a 5\n",
    ").fit(X_train_textA)\n",
    "\n",
    "X_train_cvectorized = cvectorizer.transform(X_train_textA).toarray()\n",
    "print(X_train_cvectorized.shape)\n",
    "\n",
    "X_test_cvectorized = cvectorizer.transform(X_test_textA).toarray()\n",
    "print(X_test_cvectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4469, 1775)\n",
      "(500, 1775)\n"
     ]
    }
   ],
   "source": [
    "tvectorizer = TfidfVectorizer(\n",
    "    # lowercase=True,\n",
    "    #stop_words=[word.decode('utf-8') for word in nltk.corpus.stopwords.words('spanish')],\n",
    "    #token_pattern=r'\\b\\w+\\b', #selects tokens of 2 or more alphanumeric characters \n",
    "    ngram_range=(1,3),#n-grams de palabras n = 1 a n = 3 (unigramas, bigramas y trigramas)\n",
    "    min_df=5,#ignorando los términos que tienen una frecuencia de documento estrictamente inferior a 5\n",
    ").fit(X_train_textA)\n",
    "\n",
    "X_train_tvectorized = tvectorizer.transform(X_train_textA).toarray()\n",
    "print(X_train_tvectorized.shape)\n",
    "\n",
    "X_test_tvectorized = tvectorizer.transform(X_test_textA).toarray()\n",
    "print(X_test_tvectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Perceptrón\n",
    "   \n",
    "   https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrón Multicapa (Multi-Layer Perceptron, MLP) \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=500, alpha=0.0001,\n",
    "                    solver='adam', random_state=21,tol=0.000000001)\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(6,6,6,6),solver='lbfgs',max_iter=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Accuracy mlp1 cv 0.756\n",
      "\t F1-score mlp1 cv 0.7288888888888888\n",
      "\t Accuracy mlp1 tfidv 0.74\n",
      "\t F1-score mlp1 tfidv 0.7045454545454545\n"
     ]
    }
   ],
   "source": [
    "mlp1.fit( X_train_cvectorized, y_train_hsA)\n",
    "predictions1 = mlp1.predict(X_test_cvectorized)\n",
    "\n",
    "print('\\t', 'Accuracy mlp1 cv', accuracy_score(y_test_hsA, predictions1))\n",
    "print('\\t', 'F1-score mlp1 cv', f1_score(y_test_hsA, predictions1))\n",
    "\n",
    "######\n",
    "\n",
    "mlp1.fit( X_train_tvectorized, y_train_hsA)\n",
    "predictions1 = mlp1.predict(X_test_tvectorized)\n",
    "\n",
    "print('\\t', 'Accuracy mlp1 tfidv', accuracy_score(y_test_hsA, predictions1))\n",
    "print('\\t', 'F1-score mlp1 tfidv', f1_score(y_test_hsA, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Accuracy mlp2 cv 0.74\n",
      "\t F1-score mlp2 cv 0.7072072072072072\n",
      "\t Accuracy mlp2 tfidv 0.76\n",
      "\t F1-score mlp2 tfidv 0.7435897435897435\n"
     ]
    }
   ],
   "source": [
    "mlp2.fit( X_train_cvectorized, y_train_hsA)\n",
    "predictions2 = mlp2.predict(X_test_cvectorized)\n",
    "\n",
    "print('\\t', 'Accuracy mlp2 cv', accuracy_score(y_test_hsA, predictions2))\n",
    "print('\\t', 'F1-score mlp2 cv', f1_score(y_test_hsA, predictions2))\n",
    "\n",
    "######\n",
    "\n",
    "mlp2.fit( X_train_tvectorized, y_train_hsA)\n",
    "predictions2 = mlp2.predict(X_test_tvectorized)\n",
    "\n",
    "print('\\t', 'Accuracy mlp2 tfidv', accuracy_score(y_test_hsA, predictions2))\n",
    "print('\\t', 'F1-score mlp2 tfidv', f1_score(y_test_hsA, predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las RN son sensibles a la escala de los datos de entrada, especialmente cuando se utilizan las funciones de activación sigmoide (por defecto) o tanh. Puede ser una buena práctica reescalar los datos al rango de 0 a 1, también llamado normalización. Podemos normalizar fácilmente el conjunto de datos utilizando la clase de preprocesamiento MinMaxScaler de la biblioteca scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizacion de los datos\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = scaler.fit_transform(X_train_cvectorized)\n",
    "X_test = scaler.fit_transform(X_test_cvectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Accuracy mlp2 0.784\n",
      "\t F1-score mlp2 0.7589285714285714\n"
     ]
    }
   ],
   "source": [
    "mlp2.fit( X_train, y_train_hsA)\n",
    "predictions2 = mlp2.predict(X_test)\n",
    "\n",
    "print('\\t', 'Accuracy mlp2', accuracy_score(y_test_hsA, predictions2))\n",
    "print('\\t', 'F1-score mlp2', f1_score(y_test_hsA, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales recurrentes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>¿Qué es Keras?</b>\n",
    "\n",
    "Keras es una biblioteca de Python minimalista para Deep Learning que puede funcionar sobre Theano o TensorFlow. Fue desarrollada con el objetivo de que los modelos de Deep Learning sean tan rápidos y fáciles tanto para la investigación como el desarrollo. Funciona en Python 2.7 o 3.6 y se puede ejecutar sin problemas sobre las GPU y las CPU. Es libre bajo una licencia del MIT. Keras fue desarrollado por Francois Chollet, un ingeniero de Google que utiliza cuatro principios rectores:\n",
    "<ul>\n",
    "    <li><b>Modularidad</b>: Un modelo puede entenderse sólo como una secuencia o como un gráfico. Todas las características de un modelo de aprendizaje profundo son componentes discretos que pueden combinarse de manera arbitraria.</li>\n",
    "    <li><b>Minimalismo</b>: La biblioteca proporciona lo justo para lograr un resultado, sin florituras y maximizando la legibilidad.</li>\n",
    "    <li><b>Extensibilidad</b>: Los nuevos componentes son intencionalmente fáciles de añadir y usar dentro del marco destinado a que los desarrolladores prueben y exploren nuevas ideas.</li>\n",
    "    <li><b>Python</b>: No hay modelos separados con formatos personalizados. Todo es nativo de Python.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<b>Construya modelos de Deep Learning con Keras</b>\n",
    "\n",
    "El enfoque de Keras es la idea de un modelo. El tipo principal de modelo es una secuencia de capas llamada Secuencial que es una pila lineal de capas. Se crea un Secuencial y se le añaden capas en el orden en que desea que se realice el cálculo. Una vez definido, se compila el modelo que utiliza el marco subyacente para optimizar el cálculo que se va a realizar. En este se puede especificar la función de pérdida y el optimizador a utilizar.\n",
    "\n",
    "Una vez compilado, el modelo debe ajustarse a los datos. Esto se puede hacer con un lote de datos o por el anillo o el régimen de entrenamiento del modelo entero. Aquí es donde ocurre todo el cálculo. Una vez entrenado, puede usar su modelo para hacer predicciones sobre nuevos datos. Podemos resumir la construcción de modelos de Deep learning en Keras de la siguiente manera:\n",
    "<ul>\n",
    "    <li><b>Define tu modelo</b>. Cree un modelo secuencial y añada capas.</li>\n",
    "    <li><b>Compila tu modelo</b>. Especifique la función de pérdida y los optimizadores y llame a compile() en el modelo.</li>\n",
    "    <li><b>Ajuste a su modelo</b>. Entrene el modelo sobre una muestra de datos llamando a la función fit() en el modelo.</li>\n",
    "    <li><b>Haga predicciones</b>. Utilice el modelo para generar predicciones sobre nuevos datos llamando a evaluate() o predict().</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4469, 1775) 4469 Secuencia de entrenamiento\n",
      "(500, 1775) 500 Secuencia de prueba\n"
     ]
    }
   ],
   "source": [
    "print( X_train_cvectorized.shape, len(y_train_hsA), 'Secuencia de entrenamiento' )\n",
    "\n",
    "print( X_test_cvectorized.shape, len(y_test_hsA), 'Secuencia de prueba' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    " \n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 322,113\n",
      "Trainable params: 322,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# La clase layer de redes RNN\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "# Como cualquier otra layer de Keras, SimpleRNN procesa lotes de secuencias Numpy.\n",
    "# La entrada es de la forma (batch_size, timesteps, input_features) en vez de (timesteps, input_features).\n",
    "# [muestras, pasos de tiempo, características]\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "max_features = 10000  # tamaño del diccionario de palabras comunes\n",
    "                      # (número de palabras a utilizar)\n",
    "maxlen = 1775         # longitud máxima de cada secuencia \n",
    "batch_size = 32\n",
    "\n",
    "# Capa embedding\n",
    "# input_dim : tamaño del vocabulario\n",
    "# output_dim: dimensión del vector al que se mapea\n",
    "#Se usa un embedding con tamaño de diccionario a los más de 10,000 y se mapean a dimensión un vector de dimensión 32\n",
    "model.add(Embedding(input_dim=max_features, output_dim=32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# RNN con una capa Embedding y una capa SimpleRNN que regresa solo una salida para cada secuencia\n",
    "\n",
    "# Resumen de la arquitectura\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3575 samples, validate on 894 samples\n",
      "Epoch 1/10\n",
      " - 24s - loss: 0.6808 - acc: 0.5846 - val_loss: 0.6730 - val_acc: 0.5783\n",
      "Epoch 2/10\n",
      " - 23s - loss: 0.6702 - acc: 0.6090 - val_loss: 0.6695 - val_acc: 0.6040\n",
      "Epoch 3/10\n",
      " - 24s - loss: 0.6648 - acc: 0.6232 - val_loss: 0.6605 - val_acc: 0.6309\n",
      "Epoch 4/10\n",
      " - 24s - loss: 0.6605 - acc: 0.6305 - val_loss: 0.6554 - val_acc: 0.6309\n",
      "Epoch 5/10\n",
      " - 22s - loss: 0.6644 - acc: 0.6106 - val_loss: 0.6936 - val_acc: 0.4553\n",
      "Epoch 6/10\n",
      " - 23s - loss: 0.6713 - acc: 0.5874 - val_loss: 0.6536 - val_acc: 0.6309\n",
      "Epoch 7/10\n",
      " - 22s - loss: 0.6512 - acc: 0.6336 - val_loss: 0.6510 - val_acc: 0.6309\n",
      "Epoch 8/10\n",
      " - 22s - loss: 0.6488 - acc: 0.6330 - val_loss: 0.6640 - val_acc: 0.6298\n",
      "Epoch 9/10\n",
      " - 24s - loss: 0.6556 - acc: 0.6255 - val_loss: 0.6491 - val_acc: 0.6309\n",
      "Epoch 10/10\n",
      " - 22s - loss: 0.6484 - acc: 0.6324 - val_loss: 0.6459 - val_acc: 0.6286\n",
      "Tiempo de entrenamiento: 232.02096223831177\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "import time\n",
    "tic = time.time()\n",
    "history = model.fit(\n",
    "    X_train_cvectorized, y_train_hsA,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "print('Tiempo de entrenamiento:', time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 62.00%\n",
      "\t Accuracy 0.62\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "# La función model.evaluate predice la salida para la entrada dada y luego calcula la función de métrica \n",
    "# especificada en model.compile y basada en y_true y y_pred y devuelve el valor de métrica calculada como salida\n",
    "\n",
    "# make predictions\n",
    "testPredict = model.predict(X_test_cvectorized)\n",
    "# model.predict simplemente devuelve el y_pred\n",
    "print('\\t', 'Accuracy', accuracy_score(y_test_hsA, testPredict.round()))\n",
    "\n",
    "# si usamos model.predict y luego calculamos las métricas uno mismo, el valor de la métrica calculada \n",
    "# debería resultar ser el mismo que model.evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo con Stack de RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 324,193\n",
      "Trainable params: 324,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3575 samples, validate on 894 samples\n",
      "Epoch 1/10\n",
      " - 18s - loss: 0.6845 - acc: 0.5648 - val_loss: 0.6791 - val_acc: 0.5884\n",
      "Epoch 2/10\n",
      " - 18s - loss: 0.6708 - acc: 0.6103 - val_loss: 0.6607 - val_acc: 0.6320\n",
      "Epoch 3/10\n",
      " - 20s - loss: 0.6539 - acc: 0.6305 - val_loss: 0.6510 - val_acc: 0.6130\n",
      "Epoch 4/10\n",
      " - 20s - loss: 0.6584 - acc: 0.6199 - val_loss: 0.6599 - val_acc: 0.6309\n",
      "Epoch 5/10\n",
      " - 20s - loss: 0.6512 - acc: 0.6294 - val_loss: 0.6634 - val_acc: 0.6141\n",
      "Epoch 6/10\n",
      " - 21s - loss: 0.6526 - acc: 0.6316 - val_loss: 0.6488 - val_acc: 0.6309\n",
      "Epoch 7/10\n",
      " - 21s - loss: 0.6506 - acc: 0.6280 - val_loss: 0.6422 - val_acc: 0.6331\n",
      "Epoch 8/10\n",
      " - 20s - loss: 0.6475 - acc: 0.6294 - val_loss: 0.6436 - val_acc: 0.6309\n",
      "Epoch 9/10\n",
      " - 20s - loss: 0.6754 - acc: 0.6084 - val_loss: 0.6671 - val_acc: 0.6152\n",
      "Epoch 10/10\n",
      " - 21s - loss: 0.6544 - acc: 0.6283 - val_loss: 0.6544 - val_acc: 0.5884\n",
      "Tiempo de entrenamiento: 198.6241729259491\n",
      "acc: 63.00%\n",
      "\t Accuracy 0.63\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo con Stack de RNNs\n",
    "\n",
    "model = Sequential()\n",
    "# Capa embedding\n",
    "# input_dim : tamaño del vocabulario\n",
    "# output_dim: dimensión del vector al que se mapea\n",
    "model.add(Embedding(input_dim=max_features, output_dim=32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop', \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "tic = time.time()\n",
    "history_stackRNN = model.fit(\n",
    "    X_train_cvectorized, y_train_hsA,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "print('Tiempo de entrenamiento:', time.time()-tic)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# make predictions\n",
    "testPredict_stackRNN = model.predict(X_test_cvectorized)\n",
    "print('\\t', 'Accuracy', accuracy_score(y_test_hsA, testPredict_stackRNN.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red de Memoria Corta a Largo Plazo (Long-Term Short Memory, LTSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número máximo de palabras a usar: 10000\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 328,353\n",
      "Trainable params: 328,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3575 samples, validate on 894 samples\n",
      "Epoch 1/6\n",
      " - 28s - loss: 0.6812 - acc: 0.5801 - val_loss: 0.6807 - val_acc: 0.5783\n",
      "Epoch 2/6\n",
      " - 29s - loss: 0.6771 - acc: 0.5913 - val_loss: 0.6808 - val_acc: 0.5783\n",
      "Epoch 3/6\n",
      " - 31s - loss: 0.6769 - acc: 0.5913 - val_loss: 0.6812 - val_acc: 0.5783\n",
      "Epoch 4/6\n",
      " - 30s - loss: 0.6773 - acc: 0.5913 - val_loss: 0.6813 - val_acc: 0.5783\n",
      "Epoch 5/6\n",
      " - 30s - loss: 0.6760 - acc: 0.5913 - val_loss: 0.6841 - val_acc: 0.5783\n",
      "Epoch 6/6\n",
      " - 31s - loss: 0.6757 - acc: 0.5913 - val_loss: 0.6797 - val_acc: 0.5783\n",
      "tiempo de entrenamiento:  178.82013750076294\n",
      "acc: 55.60%\n",
      "\t Accuracy 0.556\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "print('Número máximo de palabras a usar:', max_features)\n",
    "\n",
    "# Creamos el modelo con la incrustación (embedding)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "\n",
    "# Incluimos una capa LSTM\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# y entrenamos\n",
    "tic=time.time()\n",
    "history_LSTM = model.fit(\n",
    "    X_train_cvectorized, y_train_hsA,\n",
    "    epochs=6,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "print('tiempo de entrenamiento: ', time.time()-tic)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# make predictions\n",
    "testPredict_LSTM = model.predict(X_test_cvectorized)\n",
    "print('\\t', 'Accuracy', accuracy_score(y_test_hsA, testPredict_LSTM.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes con Unidades Recurrentes con Compuertas (Gated-RU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 326,273\n",
      "Trainable params: 326,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3575 samples, validate on 894 samples\n",
      "Epoch 1/10\n",
      " - 31s - loss: 0.6795 - acc: 0.5841 - val_loss: 0.6810 - val_acc: 0.5783\n",
      "Epoch 2/10\n",
      " - 35s - loss: 0.6770 - acc: 0.5913 - val_loss: 0.6813 - val_acc: 0.5783\n",
      "Epoch 3/10\n",
      " - 39s - loss: 0.6767 - acc: 0.5913 - val_loss: 0.6810 - val_acc: 0.5783\n",
      "Epoch 4/10\n",
      " - 40s - loss: 0.6768 - acc: 0.5913 - val_loss: 0.6809 - val_acc: 0.5783\n",
      "Epoch 5/10\n",
      " - 43s - loss: 0.6766 - acc: 0.5913 - val_loss: 0.6812 - val_acc: 0.5783\n",
      "Epoch 6/10\n",
      " - 39s - loss: 0.6767 - acc: 0.5913 - val_loss: 0.6811 - val_acc: 0.5783\n",
      "Epoch 7/10\n",
      " - 39s - loss: 0.6770 - acc: 0.5913 - val_loss: 0.6823 - val_acc: 0.5783\n",
      "Epoch 8/10\n",
      " - 38s - loss: 0.6771 - acc: 0.5913 - val_loss: 0.6812 - val_acc: 0.5783\n",
      "Epoch 9/10\n",
      " - 35s - loss: 0.6769 - acc: 0.5913 - val_loss: 0.6813 - val_acc: 0.5783\n",
      "Epoch 10/10\n",
      " - 37s - loss: 0.6768 - acc: 0.5913 - val_loss: 0.6809 - val_acc: 0.5783\n",
      "Tiempo de entrenamiento: 376.0974667072296\n",
      "acc: 55.60%\n",
      "\t Accuracy 0.556\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "# Capa embedding\n",
    "# input_dim : tamaño del vocabulario\n",
    "# output_dim: dimensión del vector al que se mapea\n",
    "\n",
    "model.add(Embedding(input_dim=max_features, output_dim=32))\n",
    "# comentar la siguiente linea para evaluar dropout \n",
    "model.add(GRU(32))\n",
    "# descomentar la siguiente linea para evaluar dropout \n",
    "#model.add(GRU(32, dropout=.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "tic = time.time()\n",
    "history_GRU = model.fit(\n",
    "    X_train_cvectorized, y_train_hsA,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "print('Tiempo de entrenamiento:', time.time()-tic)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# make predictions\n",
    "testPredict_GRU = model.predict(X_test_cvectorized)\n",
    "print('\\t', 'Accuracy', accuracy_score(y_test_hsA, testPredict_GRU.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos basados en atención y transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "    \n",
    "    https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n",
    "        \n",
    "        http://personal.cimat.mx:8181/~mrivera/cursos/aprendizaje_profundo/RNN_LTSM/introduccion_rnn.html\n",
    "                \n",
    "                https://unipython.com/prediccion-con-series-temporales-con-lstm-redes-neuronales-recurrentes/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
